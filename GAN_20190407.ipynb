{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Reshape\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import Flatten, Dropout\n",
    "from keras.datasets import mnist\n",
    "from keras.optimizers import Adam\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üêçmnist„Éá„Éº„ÇøË™≠„ÅøËæº„ÇÄüêç\n",
    "- Ë™≠„ÅøËæº„Çì„Å†Âæå„Å´ÁîªÂÉè„ÅÆÁîªÁ¥†ÂÄ§„Çí-1ÔΩû1„Å´Ê≠£Ë¶èÂåñ„Åó„Åü"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1c116c0c9d7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m127.5\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m127.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "(X_train, _), (_, _) = mnist.load_data()\n",
    "X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "half_batch = int(batch_size / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üêçÊúÄÈÅ©ÂåñÈñ¢Êï∞„ÅØAdam„Å´üêç\n",
    "- „Å™„Çì„ÅßAdam„Å´„Åó„Åü„ÅãÔºü[„Åì„Åì](https://www.renom.jp/ja/notebooks/tutorial/basic_algorithm/adam/notebook.html)ÂèÇÁÖß„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n",
    "- „Å®„Å¶„ÇÇÈï∑„ÅÑ„ÅÆ„ÅßÈõÜÁ¥Ñ„Åô„Çã„Å®„ÄÅÁèæÊôÇÁÇπ„Åß„ÅØÊúÄ„ÇÇÈÅ©Âàá„Å™„Éë„É©„É°„Éº„ÇøÊõ¥Êñ∞„Åå„Åß„Åç„ÇãÊúÄÈÅ©ÂåñÊâãÊ≥ï„Å®„Åó„Å¶Ë™ç„ÇÅ„Çâ„Çå„Å¶„ÅÑ„Çã„Åã„Çâ„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Kanazashi\\AppData\\Local\\conda\\conda\\envs\\TestEnv\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "z_dim = 100\n",
    "\n",
    "img_rows = 28\n",
    "img_cols = 28\n",
    "channels = 1\n",
    "img_shape = (img_rows, img_cols, channels)\n",
    "\n",
    "optimizer =Adam(0.0002, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üêçGenerator„ÇíË®≠Ë®àüêç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_shape = (z_dim,)\n",
    "generator = Sequential()\n",
    "\n",
    "generator.add(Dense(256, input_shape=noise_shape))\n",
    "generator.add(LeakyReLU(alpha=0.2))\n",
    "generator.add(BatchNormalization(momentum=0.8))\n",
    "generator.add(Dense(512))\n",
    "generator.add(LeakyReLU(alpha=0.2))\n",
    "generator.add(BatchNormalization(momentum=0.8))\n",
    "generator.add(Dense(1024))\n",
    "generator.add(LeakyReLU(alpha=0.2))\n",
    "generator.add(BatchNormalization(momentum=0.8))\n",
    "generator.add(Dense(np.prod(img_shape), activation='tanh'))\n",
    "generator.add(Reshape(img_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üêçDiscriminator„ÇíË®≠Ë®àüêç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (img_rows, img_cols, channels)\n",
    "discriminator = Sequential()\n",
    "\n",
    "discriminator.add(Flatten(input_shape=img_shape))\n",
    "discriminator.add(Dense(512))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "discriminator.add(Dense(256))\n",
    "discriminator.add(LeakyReLU(alpha=0.2))\n",
    "discriminator.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üêçcombined„ÇíË®≠Ë®àüêç\n",
    "- Generator„Å®Discriminator„Çí„Å§„Å™„ÅéÂêà„Çè„Åõ„Åü„ÇÇ„ÅÆ„Çí‰ΩúÊàê„Åó„Å¶„Åä„Åè\n",
    "- Generator„ÇíÂ≠¶Áøí„Åï„Åõ„Çã„Å®„Åç„ÅØ„ÄÅGeneratorÂçò‰Ωì„Åß„ÅØ„Å™„Åè„ÄÅ„Åì„ÅÆCombined„Çí‰Ωø„Å£„Å¶Â≠¶Áøí„ÇíË°å„ÅÜ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.trainable = False\n",
    "combined = Sequential([generator, discriminator])\n",
    "combined.compile(loss='binary_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üêçÂ≠¶Áøíüêç\n",
    "- Ë¶èÂÆöÂõûÊï∞„Åî„Å®„Å´ÁîªÂÉè„Çí‰øùÂ≠ò„Åó„Å¶„ÅÑ„Çã\n",
    "- „Å°„Çá„Å£„Å®„ÅÇ„Å®„Åß„ÇÇ„ÅÜÂ∞ë„Åó„Çè„Åã„Çä„ÇÑ„Åô„Åè„Åó„Åæ„Åô\n",
    "- ÊúÄÁµÇÁöÑ„Å´„ÅØ„ÄÅDiscriminator„ÅÆÊ≠£Á≠îÁéá„Åå50%„Å´„Å™„Çã„Åè„Çâ„ÅÑ„Åæ„Åß„ÄÅGenerator„Å´ÊàêÈï∑„Åó„Å¶„ÇÇ„Çâ„ÅÑ„Åü„ÅÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Kanazashi\\AppData\\Local\\conda\\conda\\envs\\TestEnv\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kanazashi\\AppData\\Local\\conda\\conda\\envs\\TestEnv\\lib\\site-packages\\keras\\engine\\training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.584364, acc.: 70.31%] [G loss: 0.775976]\n",
      "100 [D loss: 0.321783, acc.: 89.84%] [G loss: 2.989811]\n",
      "200 [D loss: 0.638144, acc.: 61.72%] [G loss: 1.432993]\n",
      "300 [D loss: 0.670203, acc.: 46.09%] [G loss: 0.641549]\n",
      "400 [D loss: 0.652354, acc.: 50.00%] [G loss: 0.680980]\n",
      "500 [D loss: 0.651729, acc.: 57.03%] [G loss: 0.718492]\n",
      "600 [D loss: 0.637874, acc.: 61.72%] [G loss: 0.760564]\n",
      "700 [D loss: 0.631677, acc.: 66.41%] [G loss: 0.977111]\n",
      "800 [D loss: 0.613783, acc.: 70.31%] [G loss: 0.847255]\n",
      "900 [D loss: 0.598044, acc.: 71.88%] [G loss: 0.904730]\n",
      "1000 [D loss: 0.565960, acc.: 80.47%] [G loss: 0.911945]\n",
      "1100 [D loss: 0.579055, acc.: 77.34%] [G loss: 0.935300]\n",
      "1200 [D loss: 0.594797, acc.: 71.09%] [G loss: 1.059432]\n",
      "1300 [D loss: 0.607828, acc.: 64.06%] [G loss: 0.952157]\n",
      "1400 [D loss: 0.566014, acc.: 70.31%] [G loss: 0.985570]\n",
      "1500 [D loss: 0.576758, acc.: 71.09%] [G loss: 1.025192]\n",
      "1600 [D loss: 0.622203, acc.: 61.72%] [G loss: 0.937348]\n",
      "1700 [D loss: 0.588006, acc.: 73.44%] [G loss: 0.980215]\n",
      "1800 [D loss: 0.628114, acc.: 62.50%] [G loss: 0.918821]\n",
      "1900 [D loss: 0.599472, acc.: 69.53%] [G loss: 0.943106]\n",
      "2000 [D loss: 0.623542, acc.: 64.06%] [G loss: 0.938731]\n",
      "2100 [D loss: 0.584330, acc.: 72.66%] [G loss: 0.944280]\n",
      "2200 [D loss: 0.623275, acc.: 61.72%] [G loss: 0.901478]\n",
      "2300 [D loss: 0.605445, acc.: 71.88%] [G loss: 0.934488]\n",
      "2400 [D loss: 0.680533, acc.: 56.25%] [G loss: 0.901025]\n",
      "2500 [D loss: 0.634849, acc.: 71.09%] [G loss: 0.889541]\n",
      "2600 [D loss: 0.633608, acc.: 61.72%] [G loss: 0.876870]\n",
      "2700 [D loss: 0.663632, acc.: 61.72%] [G loss: 0.860250]\n",
      "2800 [D loss: 0.638684, acc.: 57.81%] [G loss: 0.866996]\n",
      "2900 [D loss: 0.654686, acc.: 60.16%] [G loss: 0.841619]\n",
      "3000 [D loss: 0.676456, acc.: 57.03%] [G loss: 0.870467]\n",
      "3100 [D loss: 0.678821, acc.: 50.78%] [G loss: 0.877280]\n",
      "3200 [D loss: 0.661663, acc.: 62.50%] [G loss: 0.851415]\n",
      "3300 [D loss: 0.705044, acc.: 53.12%] [G loss: 0.814447]\n",
      "3400 [D loss: 0.693951, acc.: 53.91%] [G loss: 0.871835]\n",
      "3500 [D loss: 0.670789, acc.: 58.59%] [G loss: 0.866528]\n",
      "3600 [D loss: 0.662888, acc.: 57.03%] [G loss: 0.828278]\n",
      "3700 [D loss: 0.663642, acc.: 58.59%] [G loss: 0.846840]\n",
      "3800 [D loss: 0.673049, acc.: 59.38%] [G loss: 0.826506]\n",
      "3900 [D loss: 0.706817, acc.: 55.47%] [G loss: 0.809001]\n",
      "4000 [D loss: 0.665871, acc.: 62.50%] [G loss: 0.805393]\n",
      "4100 [D loss: 0.654207, acc.: 60.94%] [G loss: 0.831759]\n",
      "4200 [D loss: 0.682898, acc.: 55.47%] [G loss: 0.801743]\n",
      "4300 [D loss: 0.665103, acc.: 60.94%] [G loss: 0.808028]\n",
      "4400 [D loss: 0.684568, acc.: 53.12%] [G loss: 0.847355]\n",
      "4500 [D loss: 0.662185, acc.: 60.16%] [G loss: 0.837523]\n",
      "4600 [D loss: 0.674456, acc.: 58.59%] [G loss: 0.806894]\n",
      "4700 [D loss: 0.677596, acc.: 58.59%] [G loss: 0.834998]\n",
      "4800 [D loss: 0.702979, acc.: 48.44%] [G loss: 0.781744]\n",
      "4900 [D loss: 0.664895, acc.: 60.16%] [G loss: 0.813819]\n",
      "5000 [D loss: 0.701892, acc.: 58.59%] [G loss: 0.843928]\n",
      "5100 [D loss: 0.698305, acc.: 53.91%] [G loss: 0.823411]\n",
      "5200 [D loss: 0.672082, acc.: 56.25%] [G loss: 0.816674]\n",
      "5300 [D loss: 0.695037, acc.: 57.81%] [G loss: 0.768482]\n",
      "5400 [D loss: 0.686861, acc.: 54.69%] [G loss: 0.819060]\n",
      "5500 [D loss: 0.659123, acc.: 67.19%] [G loss: 0.822412]\n",
      "5600 [D loss: 0.689222, acc.: 57.03%] [G loss: 0.844545]\n",
      "5700 [D loss: 0.686377, acc.: 55.47%] [G loss: 0.802867]\n",
      "5800 [D loss: 0.683430, acc.: 56.25%] [G loss: 0.819166]\n",
      "5900 [D loss: 0.681379, acc.: 53.91%] [G loss: 0.845321]\n",
      "6000 [D loss: 0.709156, acc.: 50.78%] [G loss: 0.831402]\n",
      "6100 [D loss: 0.687501, acc.: 55.47%] [G loss: 0.805958]\n",
      "6200 [D loss: 0.672840, acc.: 59.38%] [G loss: 0.788354]\n",
      "6300 [D loss: 0.677785, acc.: 61.72%] [G loss: 0.836201]\n",
      "6400 [D loss: 0.715687, acc.: 53.12%] [G loss: 0.799097]\n",
      "6500 [D loss: 0.673438, acc.: 53.91%] [G loss: 0.793783]\n",
      "6600 [D loss: 0.711579, acc.: 50.78%] [G loss: 0.754988]\n",
      "6700 [D loss: 0.678137, acc.: 57.81%] [G loss: 0.827847]\n",
      "6800 [D loss: 0.642752, acc.: 64.84%] [G loss: 0.786053]\n",
      "6900 [D loss: 0.710469, acc.: 45.31%] [G loss: 0.831090]\n",
      "7000 [D loss: 0.711102, acc.: 50.78%] [G loss: 0.785352]\n",
      "7100 [D loss: 0.694269, acc.: 49.22%] [G loss: 0.789514]\n",
      "7200 [D loss: 0.679249, acc.: 53.12%] [G loss: 0.807289]\n",
      "7300 [D loss: 0.675211, acc.: 59.38%] [G loss: 0.807044]\n",
      "7400 [D loss: 0.722295, acc.: 48.44%] [G loss: 0.803930]\n",
      "7500 [D loss: 0.676052, acc.: 56.25%] [G loss: 0.808461]\n",
      "7600 [D loss: 0.677195, acc.: 52.34%] [G loss: 0.824613]\n",
      "7700 [D loss: 0.655756, acc.: 60.16%] [G loss: 0.831824]\n",
      "7800 [D loss: 0.703355, acc.: 51.56%] [G loss: 0.846846]\n",
      "7900 [D loss: 0.690539, acc.: 52.34%] [G loss: 0.798800]\n",
      "8000 [D loss: 0.688036, acc.: 53.12%] [G loss: 0.817428]\n",
      "8100 [D loss: 0.679473, acc.: 52.34%] [G loss: 0.789700]\n",
      "8200 [D loss: 0.710094, acc.: 50.00%] [G loss: 0.783954]\n",
      "8300 [D loss: 0.674760, acc.: 62.50%] [G loss: 0.817820]\n",
      "8400 [D loss: 0.708784, acc.: 48.44%] [G loss: 0.803932]\n",
      "8500 [D loss: 0.678193, acc.: 60.16%] [G loss: 0.818003]\n",
      "8600 [D loss: 0.711619, acc.: 48.44%] [G loss: 0.797330]\n",
      "8700 [D loss: 0.667004, acc.: 65.62%] [G loss: 0.805189]\n",
      "8800 [D loss: 0.681105, acc.: 61.72%] [G loss: 0.800088]\n",
      "8900 [D loss: 0.693443, acc.: 54.69%] [G loss: 0.818767]\n",
      "9000 [D loss: 0.717817, acc.: 42.19%] [G loss: 0.844406]\n",
      "9100 [D loss: 0.689479, acc.: 52.34%] [G loss: 0.801731]\n",
      "9200 [D loss: 0.681916, acc.: 56.25%] [G loss: 0.810473]\n",
      "9300 [D loss: 0.695384, acc.: 52.34%] [G loss: 0.794301]\n",
      "9400 [D loss: 0.717131, acc.: 49.22%] [G loss: 0.809430]\n",
      "9500 [D loss: 0.696815, acc.: 59.38%] [G loss: 0.795077]\n",
      "9600 [D loss: 0.666260, acc.: 61.72%] [G loss: 0.827713]\n",
      "9700 [D loss: 0.702408, acc.: 44.53%] [G loss: 0.791219]\n",
      "9800 [D loss: 0.656174, acc.: 62.50%] [G loss: 0.816464]\n",
      "9900 [D loss: 0.714491, acc.: 53.12%] [G loss: 0.804214]\n",
      "10000 [D loss: 0.678885, acc.: 57.81%] [G loss: 0.790824]\n",
      "10100 [D loss: 0.690808, acc.: 52.34%] [G loss: 0.815778]\n",
      "10200 [D loss: 0.699750, acc.: 53.12%] [G loss: 0.823767]\n",
      "10300 [D loss: 0.699609, acc.: 52.34%] [G loss: 0.812149]\n",
      "10400 [D loss: 0.671071, acc.: 54.69%] [G loss: 0.790688]\n",
      "10500 [D loss: 0.687750, acc.: 55.47%] [G loss: 0.784732]\n",
      "10600 [D loss: 0.685773, acc.: 52.34%] [G loss: 0.826133]\n",
      "10700 [D loss: 0.669217, acc.: 61.72%] [G loss: 0.789743]\n",
      "10800 [D loss: 0.729974, acc.: 42.97%] [G loss: 0.801377]\n",
      "10900 [D loss: 0.676087, acc.: 57.81%] [G loss: 0.800431]\n",
      "11000 [D loss: 0.698595, acc.: 53.91%] [G loss: 0.805851]\n",
      "11100 [D loss: 0.686752, acc.: 58.59%] [G loss: 0.780689]\n",
      "11200 [D loss: 0.714137, acc.: 46.88%] [G loss: 0.783081]\n",
      "11300 [D loss: 0.678047, acc.: 53.12%] [G loss: 0.824284]\n",
      "11400 [D loss: 0.702289, acc.: 50.78%] [G loss: 0.783585]\n",
      "11500 [D loss: 0.732474, acc.: 44.53%] [G loss: 0.803106]\n",
      "11600 [D loss: 0.696595, acc.: 48.44%] [G loss: 0.835605]\n",
      "11700 [D loss: 0.705985, acc.: 49.22%] [G loss: 0.797858]\n",
      "11800 [D loss: 0.662093, acc.: 57.03%] [G loss: 0.809777]\n",
      "11900 [D loss: 0.702108, acc.: 50.00%] [G loss: 0.824077]\n",
      "12000 [D loss: 0.684561, acc.: 50.78%] [G loss: 0.792420]\n",
      "12100 [D loss: 0.686606, acc.: 53.12%] [G loss: 0.760394]\n",
      "12200 [D loss: 0.673112, acc.: 55.47%] [G loss: 0.779157]\n",
      "12300 [D loss: 0.713606, acc.: 48.44%] [G loss: 0.753057]\n",
      "12400 [D loss: 0.708870, acc.: 53.91%] [G loss: 0.809445]\n",
      "12500 [D loss: 0.684426, acc.: 53.12%] [G loss: 0.835308]\n",
      "12600 [D loss: 0.685345, acc.: 53.91%] [G loss: 0.803300]\n",
      "12700 [D loss: 0.684423, acc.: 52.34%] [G loss: 0.745133]\n",
      "12800 [D loss: 0.685318, acc.: 55.47%] [G loss: 0.773865]\n",
      "12900 [D loss: 0.681951, acc.: 57.03%] [G loss: 0.810431]\n",
      "13000 [D loss: 0.707029, acc.: 43.75%] [G loss: 0.787194]\n",
      "13100 [D loss: 0.678156, acc.: 57.03%] [G loss: 0.841591]\n",
      "13200 [D loss: 0.687153, acc.: 54.69%] [G loss: 0.844969]\n",
      "13300 [D loss: 0.651917, acc.: 65.62%] [G loss: 0.814162]\n",
      "13400 [D loss: 0.683011, acc.: 54.69%] [G loss: 0.816458]\n",
      "13500 [D loss: 0.691747, acc.: 55.47%] [G loss: 0.760094]\n",
      "13600 [D loss: 0.673755, acc.: 59.38%] [G loss: 0.825604]\n",
      "13700 [D loss: 0.676389, acc.: 53.91%] [G loss: 0.780145]\n",
      "13800 [D loss: 0.679539, acc.: 56.25%] [G loss: 0.800688]\n",
      "13900 [D loss: 0.687352, acc.: 54.69%] [G loss: 0.828032]\n",
      "14000 [D loss: 0.677376, acc.: 54.69%] [G loss: 0.777809]\n",
      "14100 [D loss: 0.669592, acc.: 55.47%] [G loss: 0.820994]\n",
      "14200 [D loss: 0.708484, acc.: 50.78%] [G loss: 0.806696]\n",
      "14300 [D loss: 0.689516, acc.: 54.69%] [G loss: 0.804598]\n",
      "14400 [D loss: 0.716955, acc.: 50.00%] [G loss: 0.789352]\n",
      "14500 [D loss: 0.715181, acc.: 49.22%] [G loss: 0.757692]\n",
      "14600 [D loss: 0.684814, acc.: 50.78%] [G loss: 0.801345]\n",
      "14700 [D loss: 0.699066, acc.: 53.91%] [G loss: 0.801763]\n",
      "14800 [D loss: 0.655077, acc.: 60.94%] [G loss: 0.794813]\n",
      "14900 [D loss: 0.696457, acc.: 47.66%] [G loss: 0.750511]\n",
      "15000 [D loss: 0.668717, acc.: 56.25%] [G loss: 0.783143]\n",
      "15100 [D loss: 0.708954, acc.: 50.00%] [G loss: 0.791545]\n",
      "15200 [D loss: 0.680286, acc.: 51.56%] [G loss: 0.823030]\n",
      "15300 [D loss: 0.667887, acc.: 63.28%] [G loss: 0.827647]\n",
      "15400 [D loss: 0.698168, acc.: 57.03%] [G loss: 0.787661]\n",
      "15500 [D loss: 0.691595, acc.: 49.22%] [G loss: 0.817278]\n",
      "15600 [D loss: 0.677551, acc.: 64.06%] [G loss: 0.819787]\n",
      "15700 [D loss: 0.681081, acc.: 59.38%] [G loss: 0.795103]\n",
      "15800 [D loss: 0.683779, acc.: 56.25%] [G loss: 0.779556]\n",
      "15900 [D loss: 0.711103, acc.: 50.78%] [G loss: 0.792240]\n",
      "16000 [D loss: 0.687925, acc.: 53.91%] [G loss: 0.801006]\n",
      "16100 [D loss: 0.656860, acc.: 61.72%] [G loss: 0.808686]\n",
      "16200 [D loss: 0.685686, acc.: 53.12%] [G loss: 0.796545]\n",
      "16300 [D loss: 0.686330, acc.: 49.22%] [G loss: 0.790635]\n",
      "16400 [D loss: 0.746019, acc.: 48.44%] [G loss: 0.795455]\n",
      "16500 [D loss: 0.700262, acc.: 46.88%] [G loss: 0.758425]\n",
      "16600 [D loss: 0.701425, acc.: 53.91%] [G loss: 0.780434]\n",
      "16700 [D loss: 0.699553, acc.: 50.78%] [G loss: 0.809302]\n",
      "16800 [D loss: 0.681317, acc.: 53.12%] [G loss: 0.790000]\n",
      "16900 [D loss: 0.695150, acc.: 49.22%] [G loss: 0.813584]\n",
      "17000 [D loss: 0.687793, acc.: 50.78%] [G loss: 0.781013]\n",
      "17100 [D loss: 0.698541, acc.: 53.91%] [G loss: 0.766373]\n",
      "17200 [D loss: 0.676063, acc.: 57.81%] [G loss: 0.779351]\n",
      "17300 [D loss: 0.659038, acc.: 60.16%] [G loss: 0.798390]\n",
      "17400 [D loss: 0.660060, acc.: 63.28%] [G loss: 0.829958]\n",
      "17500 [D loss: 0.668161, acc.: 62.50%] [G loss: 0.781284]\n",
      "17600 [D loss: 0.683793, acc.: 52.34%] [G loss: 0.788933]\n",
      "17700 [D loss: 0.675742, acc.: 56.25%] [G loss: 0.824111]\n",
      "17800 [D loss: 0.692565, acc.: 52.34%] [G loss: 0.796981]\n",
      "17900 [D loss: 0.699988, acc.: 52.34%] [G loss: 0.792963]\n",
      "18000 [D loss: 0.671590, acc.: 60.94%] [G loss: 0.786381]\n",
      "18100 [D loss: 0.691364, acc.: 52.34%] [G loss: 0.774089]\n",
      "18200 [D loss: 0.690858, acc.: 53.12%] [G loss: 0.767559]\n",
      "18300 [D loss: 0.682955, acc.: 59.38%] [G loss: 0.802255]\n",
      "18400 [D loss: 0.701225, acc.: 50.78%] [G loss: 0.776538]\n",
      "18500 [D loss: 0.698649, acc.: 54.69%] [G loss: 0.813587]\n",
      "18600 [D loss: 0.717179, acc.: 53.12%] [G loss: 0.778850]\n",
      "18700 [D loss: 0.691456, acc.: 51.56%] [G loss: 0.836794]\n",
      "18800 [D loss: 0.711411, acc.: 51.56%] [G loss: 0.801936]\n",
      "18900 [D loss: 0.672761, acc.: 58.59%] [G loss: 0.793033]\n",
      "19000 [D loss: 0.689149, acc.: 54.69%] [G loss: 0.816859]\n",
      "19100 [D loss: 0.683858, acc.: 52.34%] [G loss: 0.788520]\n",
      "19200 [D loss: 0.728639, acc.: 45.31%] [G loss: 0.762769]\n",
      "19300 [D loss: 0.686116, acc.: 52.34%] [G loss: 0.799293]\n",
      "19400 [D loss: 0.695419, acc.: 50.00%] [G loss: 0.797091]\n",
      "19500 [D loss: 0.689758, acc.: 54.69%] [G loss: 0.796053]\n",
      "19600 [D loss: 0.694740, acc.: 53.91%] [G loss: 0.778059]\n",
      "19700 [D loss: 0.677382, acc.: 57.03%] [G loss: 0.820169]\n",
      "19800 [D loss: 0.709807, acc.: 47.66%] [G loss: 0.775975]\n",
      "19900 [D loss: 0.704739, acc.: 50.00%] [G loss: 0.806945]\n",
      "20000 [D loss: 0.716815, acc.: 48.44%] [G loss: 0.777719]\n",
      "20100 [D loss: 0.691828, acc.: 50.78%] [G loss: 0.801200]\n",
      "20200 [D loss: 0.690437, acc.: 52.34%] [G loss: 0.754079]\n",
      "20300 [D loss: 0.702480, acc.: 51.56%] [G loss: 0.792432]\n",
      "20400 [D loss: 0.689336, acc.: 53.12%] [G loss: 0.814861]\n",
      "20500 [D loss: 0.701474, acc.: 55.47%] [G loss: 0.774435]\n",
      "20600 [D loss: 0.651887, acc.: 61.72%] [G loss: 0.792483]\n",
      "20700 [D loss: 0.666420, acc.: 59.38%] [G loss: 0.770317]\n",
      "20800 [D loss: 0.688546, acc.: 57.03%] [G loss: 0.788758]\n",
      "20900 [D loss: 0.708324, acc.: 46.09%] [G loss: 0.826136]\n",
      "21000 [D loss: 0.681124, acc.: 56.25%] [G loss: 0.802859]\n",
      "21100 [D loss: 0.668414, acc.: 53.91%] [G loss: 0.803130]\n",
      "21200 [D loss: 0.706768, acc.: 48.44%] [G loss: 0.800348]\n",
      "21300 [D loss: 0.673797, acc.: 61.72%] [G loss: 0.820086]\n",
      "21400 [D loss: 0.654459, acc.: 65.62%] [G loss: 0.825498]\n",
      "21500 [D loss: 0.688824, acc.: 54.69%] [G loss: 0.794567]\n",
      "21600 [D loss: 0.667392, acc.: 59.38%] [G loss: 0.783804]\n",
      "21700 [D loss: 0.709672, acc.: 53.91%] [G loss: 0.780132]\n",
      "21800 [D loss: 0.690577, acc.: 56.25%] [G loss: 0.805668]\n",
      "21900 [D loss: 0.679231, acc.: 57.03%] [G loss: 0.795717]\n",
      "22000 [D loss: 0.668507, acc.: 60.16%] [G loss: 0.838049]\n",
      "22100 [D loss: 0.696757, acc.: 47.66%] [G loss: 0.756440]\n",
      "22200 [D loss: 0.707133, acc.: 50.00%] [G loss: 0.792940]\n",
      "22300 [D loss: 0.713617, acc.: 50.78%] [G loss: 0.810420]\n",
      "22400 [D loss: 0.698647, acc.: 54.69%] [G loss: 0.795417]\n",
      "22500 [D loss: 0.666832, acc.: 62.50%] [G loss: 0.799540]\n",
      "22600 [D loss: 0.668214, acc.: 53.91%] [G loss: 0.810298]\n",
      "22700 [D loss: 0.681597, acc.: 58.59%] [G loss: 0.795442]\n",
      "22800 [D loss: 0.661363, acc.: 62.50%] [G loss: 0.831816]\n",
      "22900 [D loss: 0.692165, acc.: 54.69%] [G loss: 0.794410]\n",
      "23000 [D loss: 0.696294, acc.: 51.56%] [G loss: 0.778094]\n",
      "23100 [D loss: 0.688711, acc.: 52.34%] [G loss: 0.757824]\n",
      "23200 [D loss: 0.701366, acc.: 53.91%] [G loss: 0.804177]\n",
      "23300 [D loss: 0.711271, acc.: 45.31%] [G loss: 0.803480]\n",
      "23400 [D loss: 0.678877, acc.: 58.59%] [G loss: 0.803412]\n",
      "23500 [D loss: 0.705264, acc.: 49.22%] [G loss: 0.786292]\n",
      "23600 [D loss: 0.710016, acc.: 50.00%] [G loss: 0.802761]\n",
      "23700 [D loss: 0.682242, acc.: 60.16%] [G loss: 0.813813]\n",
      "23800 [D loss: 0.740356, acc.: 47.66%] [G loss: 0.800700]\n",
      "23900 [D loss: 0.665649, acc.: 61.72%] [G loss: 0.801487]\n",
      "24000 [D loss: 0.684433, acc.: 57.81%] [G loss: 0.776815]\n",
      "24100 [D loss: 0.705172, acc.: 57.03%] [G loss: 0.756444]\n",
      "24200 [D loss: 0.695974, acc.: 48.44%] [G loss: 0.809495]\n",
      "24300 [D loss: 0.679343, acc.: 56.25%] [G loss: 0.798279]\n",
      "24400 [D loss: 0.682092, acc.: 56.25%] [G loss: 0.762341]\n",
      "24500 [D loss: 0.679657, acc.: 53.91%] [G loss: 0.803543]\n",
      "24600 [D loss: 0.707786, acc.: 50.78%] [G loss: 0.792626]\n",
      "24700 [D loss: 0.677762, acc.: 58.59%] [G loss: 0.818518]\n",
      "24800 [D loss: 0.702410, acc.: 44.53%] [G loss: 0.784558]\n",
      "24900 [D loss: 0.677867, acc.: 57.81%] [G loss: 0.809927]\n",
      "25000 [D loss: 0.672790, acc.: 53.91%] [G loss: 0.809173]\n",
      "25100 [D loss: 0.683933, acc.: 57.81%] [G loss: 0.804354]\n",
      "25200 [D loss: 0.695715, acc.: 55.47%] [G loss: 0.782919]\n",
      "25300 [D loss: 0.685071, acc.: 53.91%] [G loss: 0.795220]\n",
      "25400 [D loss: 0.677134, acc.: 57.03%] [G loss: 0.780496]\n",
      "25500 [D loss: 0.716713, acc.: 46.88%] [G loss: 0.797043]\n",
      "25600 [D loss: 0.690099, acc.: 49.22%] [G loss: 0.803187]\n",
      "25700 [D loss: 0.673990, acc.: 60.16%] [G loss: 0.801878]\n",
      "25800 [D loss: 0.699792, acc.: 53.12%] [G loss: 0.781522]\n",
      "25900 [D loss: 0.699168, acc.: 51.56%] [G loss: 0.777552]\n",
      "26000 [D loss: 0.665798, acc.: 61.72%] [G loss: 0.820561]\n",
      "26100 [D loss: 0.702017, acc.: 50.78%] [G loss: 0.773092]\n",
      "26200 [D loss: 0.713889, acc.: 41.41%] [G loss: 0.790856]\n",
      "26300 [D loss: 0.671906, acc.: 56.25%] [G loss: 0.791770]\n",
      "26400 [D loss: 0.696409, acc.: 50.00%] [G loss: 0.779616]\n",
      "26500 [D loss: 0.693466, acc.: 57.03%] [G loss: 0.716325]\n",
      "26600 [D loss: 0.681118, acc.: 54.69%] [G loss: 0.790086]\n",
      "26700 [D loss: 0.699957, acc.: 54.69%] [G loss: 0.777813]\n",
      "26800 [D loss: 0.679595, acc.: 55.47%] [G loss: 0.812282]\n",
      "26900 [D loss: 0.687266, acc.: 53.12%] [G loss: 0.772518]\n",
      "27000 [D loss: 0.687440, acc.: 53.12%] [G loss: 0.786651]\n",
      "27100 [D loss: 0.698362, acc.: 55.47%] [G loss: 0.799401]\n",
      "27200 [D loss: 0.703298, acc.: 48.44%] [G loss: 0.790873]\n",
      "27300 [D loss: 0.714296, acc.: 47.66%] [G loss: 0.801881]\n",
      "27400 [D loss: 0.700654, acc.: 54.69%] [G loss: 0.814608]\n",
      "27500 [D loss: 0.689191, acc.: 53.91%] [G loss: 0.782917]\n",
      "27600 [D loss: 0.698907, acc.: 50.00%] [G loss: 0.778437]\n",
      "27700 [D loss: 0.676760, acc.: 57.81%] [G loss: 0.786853]\n",
      "27800 [D loss: 0.702826, acc.: 53.91%] [G loss: 0.824035]\n",
      "27900 [D loss: 0.690880, acc.: 50.78%] [G loss: 0.807548]\n",
      "28000 [D loss: 0.681479, acc.: 50.00%] [G loss: 0.801197]\n",
      "28100 [D loss: 0.645822, acc.: 60.94%] [G loss: 0.819281]\n",
      "28200 [D loss: 0.690821, acc.: 53.91%] [G loss: 0.776856]\n",
      "28300 [D loss: 0.716390, acc.: 50.00%] [G loss: 0.799754]\n",
      "28400 [D loss: 0.696678, acc.: 49.22%] [G loss: 0.750332]\n",
      "28500 [D loss: 0.687230, acc.: 51.56%] [G loss: 0.792990]\n",
      "28600 [D loss: 0.680469, acc.: 58.59%] [G loss: 0.771972]\n",
      "28700 [D loss: 0.705634, acc.: 53.91%] [G loss: 0.801400]\n",
      "28800 [D loss: 0.653710, acc.: 62.50%] [G loss: 0.839402]\n",
      "28900 [D loss: 0.688317, acc.: 59.38%] [G loss: 0.792254]\n",
      "29000 [D loss: 0.691651, acc.: 56.25%] [G loss: 0.787523]\n",
      "29100 [D loss: 0.672174, acc.: 57.81%] [G loss: 0.811299]\n",
      "29200 [D loss: 0.684516, acc.: 59.38%] [G loss: 0.794964]\n",
      "29300 [D loss: 0.708506, acc.: 50.00%] [G loss: 0.802165]\n",
      "29400 [D loss: 0.695875, acc.: 48.44%] [G loss: 0.802087]\n",
      "29500 [D loss: 0.684802, acc.: 51.56%] [G loss: 0.773851]\n",
      "29600 [D loss: 0.677429, acc.: 58.59%] [G loss: 0.772045]\n",
      "29700 [D loss: 0.679107, acc.: 53.91%] [G loss: 0.802171]\n",
      "29800 [D loss: 0.667081, acc.: 56.25%] [G loss: 0.812974]\n",
      "29900 [D loss: 0.681234, acc.: 58.59%] [G loss: 0.806401]\n",
      "30000 [D loss: 0.686268, acc.: 55.47%] [G loss: 0.788134]\n",
      "30100 [D loss: 0.671269, acc.: 55.47%] [G loss: 0.784109]\n",
      "30200 [D loss: 0.679221, acc.: 53.91%] [G loss: 0.798682]\n",
      "30300 [D loss: 0.677911, acc.: 53.91%] [G loss: 0.816673]\n",
      "30400 [D loss: 0.694512, acc.: 50.78%] [G loss: 0.796900]\n",
      "30500 [D loss: 0.684505, acc.: 51.56%] [G loss: 0.771799]\n",
      "30600 [D loss: 0.642452, acc.: 65.62%] [G loss: 0.817470]\n",
      "30700 [D loss: 0.663541, acc.: 56.25%] [G loss: 0.815718]\n",
      "30800 [D loss: 0.690090, acc.: 52.34%] [G loss: 0.791492]\n",
      "30900 [D loss: 0.680346, acc.: 61.72%] [G loss: 0.805028]\n",
      "31000 [D loss: 0.663247, acc.: 57.03%] [G loss: 0.796731]\n",
      "31100 [D loss: 0.684072, acc.: 51.56%] [G loss: 0.790558]\n",
      "31200 [D loss: 0.651172, acc.: 59.38%] [G loss: 0.791653]\n",
      "31300 [D loss: 0.709397, acc.: 46.09%] [G loss: 0.809734]\n",
      "31400 [D loss: 0.692055, acc.: 56.25%] [G loss: 0.772613]\n",
      "31500 [D loss: 0.690549, acc.: 53.91%] [G loss: 0.792514]\n",
      "31600 [D loss: 0.682797, acc.: 56.25%] [G loss: 0.793538]\n",
      "31700 [D loss: 0.693501, acc.: 57.81%] [G loss: 0.798719]\n",
      "31800 [D loss: 0.676690, acc.: 55.47%] [G loss: 0.780557]\n",
      "31900 [D loss: 0.654794, acc.: 62.50%] [G loss: 0.757677]\n",
      "32000 [D loss: 0.681450, acc.: 53.12%] [G loss: 0.819288]\n",
      "32100 [D loss: 0.709691, acc.: 47.66%] [G loss: 0.767787]\n",
      "32200 [D loss: 0.690896, acc.: 52.34%] [G loss: 0.774810]\n",
      "32300 [D loss: 0.675146, acc.: 61.72%] [G loss: 0.797945]\n",
      "32400 [D loss: 0.661115, acc.: 63.28%] [G loss: 0.831050]\n",
      "32500 [D loss: 0.695658, acc.: 57.03%] [G loss: 0.788381]\n",
      "32600 [D loss: 0.699645, acc.: 53.91%] [G loss: 0.782108]\n",
      "32700 [D loss: 0.692711, acc.: 55.47%] [G loss: 0.794335]\n",
      "32800 [D loss: 0.704661, acc.: 52.34%] [G loss: 0.780711]\n",
      "32900 [D loss: 0.682377, acc.: 57.03%] [G loss: 0.801176]\n",
      "33000 [D loss: 0.678154, acc.: 61.72%] [G loss: 0.835837]\n",
      "33100 [D loss: 0.688555, acc.: 55.47%] [G loss: 0.765440]\n",
      "33200 [D loss: 0.674814, acc.: 60.16%] [G loss: 0.790798]\n",
      "33300 [D loss: 0.666143, acc.: 57.03%] [G loss: 0.810651]\n",
      "33400 [D loss: 0.679576, acc.: 59.38%] [G loss: 0.772257]\n",
      "33500 [D loss: 0.701584, acc.: 49.22%] [G loss: 0.780399]\n",
      "33600 [D loss: 0.714546, acc.: 49.22%] [G loss: 0.784190]\n",
      "33700 [D loss: 0.662667, acc.: 57.81%] [G loss: 0.786005]\n",
      "33800 [D loss: 0.671758, acc.: 53.91%] [G loss: 0.831270]\n",
      "33900 [D loss: 0.693345, acc.: 51.56%] [G loss: 0.780714]\n",
      "34000 [D loss: 0.690560, acc.: 57.03%] [G loss: 0.818842]\n",
      "34100 [D loss: 0.691104, acc.: 53.91%] [G loss: 0.796163]\n",
      "34200 [D loss: 0.663530, acc.: 59.38%] [G loss: 0.801188]\n",
      "34300 [D loss: 0.686384, acc.: 53.91%] [G loss: 0.801803]\n",
      "34400 [D loss: 0.697040, acc.: 52.34%] [G loss: 0.801552]\n",
      "34500 [D loss: 0.691634, acc.: 48.44%] [G loss: 0.753489]\n",
      "34600 [D loss: 0.669575, acc.: 59.38%] [G loss: 0.798453]\n",
      "34700 [D loss: 0.688778, acc.: 55.47%] [G loss: 0.759299]\n",
      "34800 [D loss: 0.691228, acc.: 53.91%] [G loss: 0.779384]\n",
      "34900 [D loss: 0.671692, acc.: 52.34%] [G loss: 0.774366]\n",
      "35000 [D loss: 0.693135, acc.: 48.44%] [G loss: 0.817630]\n",
      "35100 [D loss: 0.693180, acc.: 57.81%] [G loss: 0.790659]\n",
      "35200 [D loss: 0.677282, acc.: 53.91%] [G loss: 0.830958]\n",
      "35300 [D loss: 0.683460, acc.: 55.47%] [G loss: 0.802900]\n",
      "35400 [D loss: 0.685196, acc.: 51.56%] [G loss: 0.794135]\n",
      "35500 [D loss: 0.679318, acc.: 57.03%] [G loss: 0.814952]\n",
      "35600 [D loss: 0.679988, acc.: 59.38%] [G loss: 0.800867]\n",
      "35700 [D loss: 0.685165, acc.: 57.03%] [G loss: 0.762399]\n",
      "35800 [D loss: 0.698116, acc.: 55.47%] [G loss: 0.780616]\n",
      "35900 [D loss: 0.677533, acc.: 54.69%] [G loss: 0.778181]\n",
      "36000 [D loss: 0.665076, acc.: 61.72%] [G loss: 0.766664]\n",
      "36100 [D loss: 0.701486, acc.: 50.78%] [G loss: 0.800008]\n",
      "36200 [D loss: 0.667970, acc.: 59.38%] [G loss: 0.827071]\n",
      "36300 [D loss: 0.694999, acc.: 53.91%] [G loss: 0.791142]\n",
      "36400 [D loss: 0.689565, acc.: 53.12%] [G loss: 0.810527]\n",
      "36500 [D loss: 0.696825, acc.: 55.47%] [G loss: 0.779858]\n",
      "36600 [D loss: 0.682938, acc.: 57.03%] [G loss: 0.790844]\n",
      "36700 [D loss: 0.674215, acc.: 54.69%] [G loss: 0.784985]\n",
      "36800 [D loss: 0.684189, acc.: 55.47%] [G loss: 0.785040]\n",
      "36900 [D loss: 0.684841, acc.: 52.34%] [G loss: 0.780792]\n",
      "37000 [D loss: 0.687504, acc.: 51.56%] [G loss: 0.780822]\n",
      "37100 [D loss: 0.671363, acc.: 58.59%] [G loss: 0.786557]\n",
      "37200 [D loss: 0.691520, acc.: 51.56%] [G loss: 0.771418]\n",
      "37300 [D loss: 0.690131, acc.: 56.25%] [G loss: 0.803214]\n",
      "37400 [D loss: 0.696767, acc.: 53.12%] [G loss: 0.773056]\n",
      "37500 [D loss: 0.655115, acc.: 56.25%] [G loss: 0.800821]\n",
      "37600 [D loss: 0.679325, acc.: 56.25%] [G loss: 0.792420]\n",
      "37700 [D loss: 0.698522, acc.: 53.91%] [G loss: 0.764035]\n",
      "37800 [D loss: 0.724440, acc.: 42.19%] [G loss: 0.800028]\n",
      "37900 [D loss: 0.695363, acc.: 56.25%] [G loss: 0.786385]\n",
      "38000 [D loss: 0.652483, acc.: 62.50%] [G loss: 0.788882]\n",
      "38100 [D loss: 0.650236, acc.: 64.06%] [G loss: 0.807664]\n",
      "38200 [D loss: 0.661030, acc.: 58.59%] [G loss: 0.799379]\n",
      "38300 [D loss: 0.674906, acc.: 54.69%] [G loss: 0.855300]\n",
      "38400 [D loss: 0.681625, acc.: 57.03%] [G loss: 0.795765]\n",
      "38500 [D loss: 0.694140, acc.: 53.91%] [G loss: 0.834758]\n",
      "38600 [D loss: 0.699052, acc.: 49.22%] [G loss: 0.814512]\n",
      "38700 [D loss: 0.669347, acc.: 60.16%] [G loss: 0.802539]\n",
      "38800 [D loss: 0.709702, acc.: 53.91%] [G loss: 0.779188]\n",
      "38900 [D loss: 0.671050, acc.: 57.81%] [G loss: 0.796897]\n",
      "39000 [D loss: 0.684108, acc.: 50.78%] [G loss: 0.774696]\n",
      "39100 [D loss: 0.728845, acc.: 43.75%] [G loss: 0.802509]\n",
      "39200 [D loss: 0.715310, acc.: 53.12%] [G loss: 0.774104]\n",
      "39300 [D loss: 0.690250, acc.: 54.69%] [G loss: 0.802863]\n",
      "39400 [D loss: 0.649213, acc.: 65.62%] [G loss: 0.822009]\n",
      "39500 [D loss: 0.656290, acc.: 60.94%] [G loss: 0.785301]\n",
      "39600 [D loss: 0.689088, acc.: 53.91%] [G loss: 0.796245]\n",
      "39700 [D loss: 0.681421, acc.: 58.59%] [G loss: 0.835689]\n",
      "39800 [D loss: 0.662065, acc.: 58.59%] [G loss: 0.803291]\n",
      "39900 [D loss: 0.674491, acc.: 55.47%] [G loss: 0.783124]\n",
      "40000 [D loss: 0.711571, acc.: 50.00%] [G loss: 0.821751]\n",
      "40100 [D loss: 0.693120, acc.: 53.12%] [G loss: 0.808766]\n",
      "40200 [D loss: 0.734926, acc.: 48.44%] [G loss: 0.789502]\n",
      "40300 [D loss: 0.681433, acc.: 50.78%] [G loss: 0.798118]\n",
      "40400 [D loss: 0.677843, acc.: 58.59%] [G loss: 0.820948]\n",
      "40500 [D loss: 0.674771, acc.: 58.59%] [G loss: 0.821663]\n",
      "40600 [D loss: 0.678552, acc.: 55.47%] [G loss: 0.785186]\n",
      "40700 [D loss: 0.708980, acc.: 52.34%] [G loss: 0.783166]\n",
      "40800 [D loss: 0.694732, acc.: 57.81%] [G loss: 0.805558]\n",
      "40900 [D loss: 0.694815, acc.: 53.12%] [G loss: 0.813635]\n",
      "41000 [D loss: 0.708312, acc.: 49.22%] [G loss: 0.797307]\n",
      "41100 [D loss: 0.665255, acc.: 53.91%] [G loss: 0.835305]\n",
      "41200 [D loss: 0.686214, acc.: 50.00%] [G loss: 0.810245]\n",
      "41300 [D loss: 0.680546, acc.: 53.91%] [G loss: 0.798798]\n",
      "41400 [D loss: 0.694875, acc.: 55.47%] [G loss: 0.790065]\n",
      "41500 [D loss: 0.661152, acc.: 60.94%] [G loss: 0.809626]\n",
      "41600 [D loss: 0.664297, acc.: 59.38%] [G loss: 0.812173]\n",
      "41700 [D loss: 0.689560, acc.: 52.34%] [G loss: 0.749589]\n",
      "41800 [D loss: 0.677301, acc.: 53.12%] [G loss: 0.818247]\n",
      "41900 [D loss: 0.672267, acc.: 55.47%] [G loss: 0.795800]\n",
      "42000 [D loss: 0.684603, acc.: 56.25%] [G loss: 0.798392]\n",
      "42100 [D loss: 0.676655, acc.: 56.25%] [G loss: 0.797226]\n",
      "42200 [D loss: 0.676562, acc.: 59.38%] [G loss: 0.788466]\n",
      "42300 [D loss: 0.695436, acc.: 57.81%] [G loss: 0.800493]\n",
      "42400 [D loss: 0.693894, acc.: 50.00%] [G loss: 0.771099]\n",
      "42500 [D loss: 0.694417, acc.: 53.91%] [G loss: 0.784672]\n",
      "42600 [D loss: 0.674675, acc.: 56.25%] [G loss: 0.817875]\n",
      "42700 [D loss: 0.647319, acc.: 65.62%] [G loss: 0.769783]\n",
      "42800 [D loss: 0.683840, acc.: 54.69%] [G loss: 0.795221]\n",
      "42900 [D loss: 0.657894, acc.: 64.06%] [G loss: 0.814730]\n",
      "43000 [D loss: 0.672876, acc.: 54.69%] [G loss: 0.830061]\n",
      "43100 [D loss: 0.691703, acc.: 54.69%] [G loss: 0.789589]\n",
      "43200 [D loss: 0.672604, acc.: 53.91%] [G loss: 0.807333]\n",
      "43300 [D loss: 0.705052, acc.: 42.97%] [G loss: 0.798300]\n",
      "43400 [D loss: 0.712350, acc.: 47.66%] [G loss: 0.793499]\n",
      "43500 [D loss: 0.658372, acc.: 61.72%] [G loss: 0.806512]\n",
      "43600 [D loss: 0.691893, acc.: 50.78%] [G loss: 0.797388]\n",
      "43700 [D loss: 0.700365, acc.: 52.34%] [G loss: 0.786413]\n",
      "43800 [D loss: 0.671998, acc.: 51.56%] [G loss: 0.789474]\n",
      "43900 [D loss: 0.679817, acc.: 58.59%] [G loss: 0.798463]\n",
      "44000 [D loss: 0.720214, acc.: 50.00%] [G loss: 0.811504]\n",
      "44100 [D loss: 0.667588, acc.: 61.72%] [G loss: 0.784678]\n",
      "44200 [D loss: 0.636840, acc.: 63.28%] [G loss: 0.828244]\n",
      "44300 [D loss: 0.664699, acc.: 59.38%] [G loss: 0.785417]\n",
      "44400 [D loss: 0.727062, acc.: 50.78%] [G loss: 0.799320]\n",
      "44500 [D loss: 0.663169, acc.: 59.38%] [G loss: 0.813049]\n",
      "44600 [D loss: 0.681951, acc.: 59.38%] [G loss: 0.833408]\n",
      "44700 [D loss: 0.684860, acc.: 57.03%] [G loss: 0.798215]\n",
      "44800 [D loss: 0.646515, acc.: 61.72%] [G loss: 0.796404]\n",
      "44900 [D loss: 0.676797, acc.: 57.03%] [G loss: 0.769465]\n",
      "45000 [D loss: 0.668058, acc.: 57.03%] [G loss: 0.820742]\n",
      "45100 [D loss: 0.683512, acc.: 57.03%] [G loss: 0.777964]\n",
      "45200 [D loss: 0.699502, acc.: 54.69%] [G loss: 0.798914]\n",
      "45300 [D loss: 0.685304, acc.: 53.12%] [G loss: 0.780421]\n",
      "45400 [D loss: 0.660451, acc.: 57.81%] [G loss: 0.805786]\n",
      "45500 [D loss: 0.677856, acc.: 53.91%] [G loss: 0.764096]\n",
      "45600 [D loss: 0.704665, acc.: 52.34%] [G loss: 0.812969]\n",
      "45700 [D loss: 0.681246, acc.: 53.91%] [G loss: 0.820585]\n",
      "45800 [D loss: 0.694299, acc.: 48.44%] [G loss: 0.811665]\n",
      "45900 [D loss: 0.693568, acc.: 57.81%] [G loss: 0.786444]\n",
      "46000 [D loss: 0.652788, acc.: 59.38%] [G loss: 0.801173]\n",
      "46100 [D loss: 0.695209, acc.: 56.25%] [G loss: 0.793418]\n",
      "46200 [D loss: 0.668936, acc.: 51.56%] [G loss: 0.777222]\n",
      "46300 [D loss: 0.699931, acc.: 53.12%] [G loss: 0.800657]\n",
      "46400 [D loss: 0.684263, acc.: 53.91%] [G loss: 0.788596]\n",
      "46500 [D loss: 0.669345, acc.: 58.59%] [G loss: 0.827247]\n",
      "46600 [D loss: 0.693184, acc.: 51.56%] [G loss: 0.808084]\n",
      "46700 [D loss: 0.666360, acc.: 55.47%] [G loss: 0.754301]\n",
      "46800 [D loss: 0.689764, acc.: 52.34%] [G loss: 0.785437]\n",
      "46900 [D loss: 0.698953, acc.: 57.03%] [G loss: 0.792124]\n",
      "47000 [D loss: 0.682836, acc.: 54.69%] [G loss: 0.799545]\n",
      "47100 [D loss: 0.701492, acc.: 51.56%] [G loss: 0.774723]\n",
      "47200 [D loss: 0.660201, acc.: 60.16%] [G loss: 0.834378]\n",
      "47300 [D loss: 0.687315, acc.: 60.16%] [G loss: 0.798209]\n",
      "47400 [D loss: 0.692560, acc.: 57.81%] [G loss: 0.809562]\n",
      "47500 [D loss: 0.730226, acc.: 42.97%] [G loss: 0.805659]\n",
      "47600 [D loss: 0.676174, acc.: 59.38%] [G loss: 0.849631]\n",
      "47700 [D loss: 0.681213, acc.: 54.69%] [G loss: 0.792991]\n",
      "47800 [D loss: 0.675192, acc.: 56.25%] [G loss: 0.807817]\n",
      "47900 [D loss: 0.696955, acc.: 57.81%] [G loss: 0.790471]\n",
      "48000 [D loss: 0.660026, acc.: 58.59%] [G loss: 0.816011]\n",
      "48100 [D loss: 0.662631, acc.: 53.12%] [G loss: 0.807510]\n",
      "48200 [D loss: 0.687580, acc.: 54.69%] [G loss: 0.780415]\n",
      "48300 [D loss: 0.708297, acc.: 50.78%] [G loss: 0.807990]\n",
      "48400 [D loss: 0.658480, acc.: 60.94%] [G loss: 0.798557]\n",
      "48500 [D loss: 0.674138, acc.: 52.34%] [G loss: 0.788619]\n",
      "48600 [D loss: 0.687639, acc.: 53.12%] [G loss: 0.779480]\n",
      "48700 [D loss: 0.709863, acc.: 49.22%] [G loss: 0.817309]\n",
      "48800 [D loss: 0.689849, acc.: 53.12%] [G loss: 0.813645]\n",
      "48900 [D loss: 0.662578, acc.: 60.16%] [G loss: 0.817463]\n",
      "49000 [D loss: 0.663645, acc.: 62.50%] [G loss: 0.838864]\n",
      "49100 [D loss: 0.674666, acc.: 55.47%] [G loss: 0.775692]\n",
      "49200 [D loss: 0.634564, acc.: 65.62%] [G loss: 0.799413]\n",
      "49300 [D loss: 0.652749, acc.: 62.50%] [G loss: 0.790045]\n",
      "49400 [D loss: 0.674901, acc.: 51.56%] [G loss: 0.818386]\n",
      "49500 [D loss: 0.680611, acc.: 57.03%] [G loss: 0.839105]\n",
      "49600 [D loss: 0.699333, acc.: 47.66%] [G loss: 0.781953]\n",
      "49700 [D loss: 0.670956, acc.: 60.94%] [G loss: 0.772760]\n",
      "49800 [D loss: 0.663641, acc.: 60.94%] [G loss: 0.823889]\n",
      "49900 [D loss: 0.669054, acc.: 53.91%] [G loss: 0.808523]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50000):\n",
    "    \n",
    "    # ‰ª•‰∏ã„ÄÅDiscriminatorÂ≠¶Áøí\n",
    "    # Generator„Å´„Çà„ÇãÂÅΩ„Éá„Éº„Çø„ÅÆÁîüÊàê\n",
    "    noise = np.random.normal(0, 1, (half_batch, z_dim))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "    \n",
    "    # „Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫„ÅÆÂçäÊï∞„ÇíÊïôÂ∏´„Éá„Éº„Çø„Åã„Çâ„Éî„ÉÉ„ÇØ„Ç¢„ÉÉ„Éó\n",
    "    idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "    imgs = X_train[idx]\n",
    "    \n",
    "    # discriminatorÂ≠¶Áøí\n",
    "    d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
    "    d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "    \n",
    "    # „Åù„Çå„Åû„Çå„ÅÆÊêçÂ§±Èñ¢Êï∞„ÇíÂπ≥Âùá\n",
    "    d_loss_mean = np.add(d_loss_real, d_loss_fake) * 0.5\n",
    "    \n",
    "    # ‰ª•‰∏ã„ÄÅGeneratorÂ≠¶Áøí\n",
    "    noise = np.random.normal(0, 1, (batch_size, z_dim))\n",
    "    g_loss = combined.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        gen_imgs = np.squeeze(gen_imgs)\n",
    "        save_img = gen_imgs[0] * 127.5 + 127.5\n",
    "        Image.fromarray(save_img.astype(np.uint8)).save('savefig/{}.png'.format(str(epoch)))\n",
    "        print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss_mean[0], 100*d_loss_mean[1], g_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üêç„É¢„Éá„É´„ÅÆ‰øùÂ≠ò„ÇíË°å„ÅÜ\n",
    "- generator, discriminatorÂÖ±„Å´„ÄÅ„É¢„Éá„É´„ÅÆÊßãÈÄ†Ëá™‰Ωì„Çí.json„Åß‰øùÂ≠ò„Åó„ÄÅÈáç„Åø„ÅØ.h5„Éï„Ç°„Ç§„É´„Åß‰øùÂ≠ò"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_json_string = generator.to_json()\n",
    "\n",
    "with open('saved_model/generator_model.json', 'w') as fw_g:\n",
    "    fw_g.write(generator_json_string)\n",
    "\n",
    "generator.save_weights('saved_model/generator_weights.h5')\n",
    "\n",
    "discriminator_json_string = discriminator.to_json()\n",
    "\n",
    "with open('saved_model/discriminator_model.json', 'w') as fw_d:\n",
    "    fw_d.write(discriminator_json_string)\n",
    "\n",
    "discriminator.save_weights('saved_model/discriminator_weights.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
